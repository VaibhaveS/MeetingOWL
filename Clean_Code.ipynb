{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8417ca96",
   "metadata": {},
   "source": [
    "### Installing and Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfaf798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from azure.cognitiveservices.speech import AudioDataStream, SpeechConfig, SpeechSynthesizer, SpeechSynthesisOutputFormat\n",
    "from azure.cognitiveservices.speech.audio import AudioOutputConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk.corpus\n",
    "from nltk.corpus import nps_chat\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('nps_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32295f",
   "metadata": {},
   "source": [
    "### Making necessary connections with Google API and Anvil server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f4025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Connected to \"Default environment\" as SERVER\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"bionic-genre-326715-416d6d51b533.json\"\n",
    "import anvil.server\n",
    "anvil.server.connect(\"XBQM36YNDUZ4TWI522HM4KDL-NUKYRRNENRSFFQ4A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c1d04",
   "metadata": {},
   "source": [
    "### Speech to text conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd2b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_bucket(blob_name, path_to_file, bucket_name):\n",
    "    \"\"\" Upload data to a bucket\"\"\"\n",
    "\n",
    "    # Explicitly use service account credentials by specifying the private key\n",
    "    # file.\n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        'bionic-genre-326715-416d6d51b533.json')\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(path_to_file)\n",
    "\n",
    "    #returns a public url\n",
    "    return blob.public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05707883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_blob(blob_name, bucket_name):\n",
    "    \"\"\"Deletes a blob from the bucket.\"\"\"\n",
    "\n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        'bionic-genre-326715-416d6d51b533.json')\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6811c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "def speech_to_text(file_path):\n",
    "    \n",
    "    from google.cloud import speech\n",
    "\n",
    "    gcs_uri=upload_to_bucket(\"MeetingOWL\",file_path,\"nlp_project_13\")\n",
    "    # Instantiates a client\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    # The name of the audio file to transcribe\n",
    "    gcs_uri = \"gs://nlp_project_13/MeetingOWL\"\n",
    "\n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "\n",
    "    config = speech.RecognitionConfig(\n",
    "        #encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        encoding= speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-IN\",enable_word_time_offsets=True,enable_automatic_punctuation=True,\n",
    "    )\n",
    "\n",
    "    # Detects speech in the audio file\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    word_stamps=[]\n",
    "    transcript=[]\n",
    "    \n",
    "    for result in response.results:\n",
    "        alternative = result.alternatives[0]\n",
    "        for word_info in alternative.words:\n",
    "            word = word_info.word\n",
    "            start_time = word_info.start_time\n",
    "            end_time = word_info.end_time\n",
    "            word_stamps.append([word,start_time,end_time])\n",
    "        transcript.append(alternative.transcript)\n",
    "    delete_blob(\"MeetingOWL\",\"nlp_project_13\")\n",
    "    return [transcript,word_stamps]\n",
    "   # return response.results \n",
    "#speech_to_text(\"finally.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e1d8b",
   "metadata": {},
   "source": [
    "### Writing the transcript on to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e4beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transcript(transcript):\n",
    "    file1 = open('myfile.txt', 'w')\n",
    "    file1.writelines(transcript)\n",
    "    file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d37473",
   "metadata": {},
   "source": [
    "### Tokenization, lemmatization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9309ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    words = word_tokenize(doc)\n",
    "    \n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n",
    "    \n",
    "    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n",
    "    \n",
    "    lemmas = []\n",
    "    for w in non_punctuation:\n",
    "        if w[1].startswith('J'):\n",
    "            pos = wordnet.ADJ\n",
    "        elif w[1].startswith('V'):\n",
    "            pos = wordnet.VERB\n",
    "        elif w[1].startswith('N'):\n",
    "            pos = wordnet.NOUN\n",
    "        elif w[1].startswith('R'):\n",
    "            pos = wordnet.ADV\n",
    "        else:\n",
    "            pos = wordnet.NOUN\n",
    "        \n",
    "        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402960c",
   "metadata": {},
   "source": [
    "### Question detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ae1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class isQuestion():\n",
    "    def __init__(self):\n",
    "        posts = nltk.corpus.nps_chat.xml_posts()\n",
    "        features = self.__get_feature_set(posts)\n",
    "        self.classifier = self.naiveBayes(features)\n",
    "    def __get_feature_set(self, posts):\n",
    "        feature = []\n",
    "        for post in posts:\n",
    "            post_text = post.text            \n",
    "            features = {}\n",
    "            words = nltk.word_tokenize(post_text)\n",
    "            for word in words:\n",
    "                features['contains({})'.format(word.lower())] = True\n",
    "            feature.append((features, post.get('class')))\n",
    "        return feature\n",
    "    def naiveBayes(self, feature_set):\n",
    "        ## Creates a Naive Bayes model using the words and the tags of words present in the nps_chat corpus\n",
    "        training_size = int(len(feature_set) * 0.1)\n",
    "        train_set, test_set = feature_set[training_size:], feature_set[:training_size]\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        return classifier\n",
    "    def predict_question(self, text):\n",
    "        ## Given a list of sentences, it returns a list of sentences which are questions using naive bayes model\n",
    "        words = nltk.word_tokenize(text.lower())        \n",
    "        if '?' in text:\n",
    "            return 1\n",
    "        features = {}\n",
    "        for word in words:\n",
    "            features['contains({})'.format(word.lower())] = True            \n",
    "        prediction_result = self.classifier.classify(features)\n",
    "        if prediction_result == 'whQuestion' or prediction_result == 'ynQuestion':\n",
    "            return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4570f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the list of questions\n",
    "def Question_detection(transcript):\n",
    "    result = []\n",
    "    obj = isQuestion()\n",
    "    for i in transcript:\n",
    "        if(obj.predict_question(i) == 1):\n",
    "            result.append(i)\n",
    "    return result\n",
    "    #transcript is a list of setences ex. [\"hello everyone\",\"how is everyone\",\"lets start todays class\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ec879",
   "metadata": {},
   "source": [
    "### Finding the timestamps of silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba44b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silience(file_name):\n",
    "    from pydub import AudioSegment,silence\n",
    "    myaudio = intro = AudioSegment.from_wav(file_name)\n",
    "    silence = silence.detect_silence(myaudio, min_silence_len=1000, silence_thresh=-16)\n",
    "    silence = [((start/1000),(stop/1000)) for start,stop in silence] #convert to sec\n",
    "    return silence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24391e50",
   "metadata": {},
   "source": [
    "### Question difficulty classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4610dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question,all_data,k,threshold,tfidf_vectorizer,tfidf_matrix):\n",
    "    query_vect = tfidf_vectorizer.transform([question])\n",
    "    similarity = cosine_similarity(query_vect, tfidf_matrix)[0]\n",
    "    sim=[]\n",
    "    for i in range(len(similarity)):\n",
    "        sim.append((similarity[i],i))\n",
    "    sim.sort(reverse=True)\n",
    "    #use the top k results \n",
    "    interesting,difficult = 0,0\n",
    "    for max_indices in range(k):\n",
    "        max_index=sim[max_indices][1]\n",
    "        max_sim = sim[max_indices][0]\n",
    "        weight_diff = -1\n",
    "        weight_interesting = -1\n",
    "        if(all_data.iloc[max_index]['Is the question difficult?']=='Yes'): weight_diff=1\n",
    "        if(all_data.iloc[max_index]['Is the question interesting?']=='Yes'): weight_interesting=1\n",
    "        difficult+= weight_diff*max_sim\n",
    "        interesting+= weight_interesting*max_sim\n",
    "    \n",
    "    result={}\n",
    "    \n",
    "    if(difficult>=threshold):\n",
    "        result[\"difficulty\"]=\"Yes\"\n",
    "    else:\n",
    "        result[\"difficulty\"]=\"No\"\n",
    "    \n",
    "    if(interesting>=threshold):\n",
    "        result[\"interesting\"]=\"Yes\"\n",
    "    else:\n",
    "        result[\"interesting\"]=\"No\"\n",
    "        \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c502ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interesting_questions(Questions,df,tfidf_vectorizer,tfidf_matrix):\n",
    "    diff,inter=0,0\n",
    "    for question in Questions:\n",
    "        response = ask_question(question,df,1,0,tfidf_vectorizer,tfidf_matrix)\n",
    "        if(response[\"difficulty\"]==\"Yes\"): diff+=1\n",
    "        if(response[\"interesting\"]==\"Yes\"): inter+=1\n",
    "    return [diff,inter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b9019ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data():\n",
    "    #path to the form responses\n",
    "    df = pd.read_csv(\"question_feedback.csv\")\n",
    "    df.iloc[0:,1:]\n",
    "    df=df.rename(columns={'Write a question relevant to our subjects which profs ask(eg. What is NLP?, Can someone tell me the difference between ML and AI? etc.)':'Question'})\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(tuple(df['Question']))\n",
    "    return [df,tfidf_matrix,tfidf_vectorizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19cef742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.media\n",
    "from shutil import copy\n",
    "@anvil.server.callable\n",
    "def write_media_to_file(media_object):\n",
    "    with anvil.media.TempFile(media_object) as f:\n",
    "        #audio file stored in finally.mp3\n",
    "        df,tfidf_matrix,tfidf_vectorizer=init_data()\n",
    "        copy(f, \"audio.mp3\")\n",
    "        #convert the speech to text\n",
    "        transcript,word_stamps=speech_to_text(\"audio.mp3\")\n",
    "        Questions = Question_detection(transcript)\n",
    "        diff,inter=interesting_questions(Questions,df,tfidf_vectorizer,tfidf_matrix)\n",
    "        test_transcript(transcript)\n",
    "        return [len(transcript),len(Questions),diff,inter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1407d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noOfSpeakers():\n",
    "    from google.cloud import speech_v1p1beta1 as speech\n",
    "    #from google.cloud.speech import enums\n",
    "    client = speech.SpeechClient()\n",
    "    speech_file = \"audio.mpeg\"\n",
    "    with open(speech_file, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    print(\"processing...\")\n",
    "    gcs_uri = \"gs://nlp_project_13/audio.mpeg\"\n",
    "    #audio = speech.RecognitionAudio(content=content)\n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "    diarization_config = speech.SpeakerDiarizationConfig(\n",
    "      enable_speaker_diarization=True,\n",
    "      min_speaker_count=2,\n",
    "      max_speaker_count=10,\n",
    "    )\n",
    "\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding = speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED,\n",
    "        sample_rate_hertz=8000,\n",
    "        language_code=\"en-US\",\n",
    "        diarization_config=diarization_config,\n",
    "    )\n",
    "    operation = client.long_running_recognize(config=config, audio=audio)\n",
    "    response = operation.result(timeout=500)\n",
    "    for result in response.results:\n",
    "        words_info = result.alternatives[0].words\n",
    "    speakers = []\n",
    "    noOfSpeakers = 0\n",
    "    for word_info in words_info:\n",
    "        speakers.append(int(word_info.speaker_tag))\n",
    "        #noOfSpeakers = max(noOfSpeakers, int(word_info.speaker_tag))\n",
    "        print(\n",
    "            u\"word: '{}', speaker_tag: {}\".format(word_info.word, word_info.speaker_tag)\n",
    "        )\n",
    "    speakers = set(speakers)\n",
    "    return len(speakers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec8e45",
   "metadata": {},
   "source": [
    "### Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c25c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "    ## Speech to text\n",
    "    print(noOfSpeakers())\n",
    "    \"\"\"text = speech_to_text(\"audio.mpeg\")\n",
    "    print(\"Transcript of the audio file is:\\n\")\n",
    "    print(text)\"\"\"\n",
    "    questions = Question_detection([\"Hello, can you tell me what is machine learning?\",\"machine learning is a branch of Artificial Intelligence and computer science which focuses on the use of data and algorithms to imitate the way humans learn\",\"Can you tell me what are the different frameworks which are available in JavaScript?\",\"Could you tell me the founders of Google?\",\"the founders of Google are Larry Page and Sergey Brin.\"])\n",
    "    print(\"\\nList of questions from the transcript are:\\n\")\n",
    "    print(questions)\n",
    "    df_and_tfIdf = init_data()\n",
    "    Interesting_and_diff_Questions = interesting_questions(questions, df_and_tfIdf[0], df_and_tfIdf[2], df_and_tfIdf[1])\n",
    "    print(\"\\nThe number of interesting questions from the list of questions are: \" + str(Interesting_and_diff_Questions[0]))\n",
    "    print(\"\\nThe number of difficult questions from the list of questions are: \" + str(Interesting_and_diff_Questions[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44e4192f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing...\n",
      "word: 'hello', speaker_tag: 2\n",
      "word: 'can', speaker_tag: 2\n",
      "word: 'you', speaker_tag: 2\n",
      "word: 'tell', speaker_tag: 2\n",
      "word: 'me', speaker_tag: 2\n",
      "word: 'what', speaker_tag: 2\n",
      "word: 'this', speaker_tag: 2\n",
      "word: 'machine', speaker_tag: 2\n",
      "word: 'line', speaker_tag: 2\n",
      "word: 'machine', speaker_tag: 2\n",
      "word: 'learning', speaker_tag: 2\n",
      "word: 'is', speaker_tag: 2\n",
      "word: 'a', speaker_tag: 2\n",
      "word: 'branch', speaker_tag: 2\n",
      "word: 'of', speaker_tag: 2\n",
      "word: 'Activision', speaker_tag: 2\n",
      "word: 'in', speaker_tag: 2\n",
      "word: 'Pleasant', speaker_tag: 2\n",
      "word: 'and', speaker_tag: 2\n",
      "word: 'computer', speaker_tag: 2\n",
      "word: 'science', speaker_tag: 2\n",
      "word: 'which', speaker_tag: 2\n",
      "word: 'focuses', speaker_tag: 2\n",
      "word: 'on', speaker_tag: 2\n",
      "word: 'the', speaker_tag: 2\n",
      "word: 'use', speaker_tag: 2\n",
      "word: 'of', speaker_tag: 2\n",
      "word: 'data', speaker_tag: 2\n",
      "word: 'structures', speaker_tag: 2\n",
      "word: 'and', speaker_tag: 2\n",
      "word: 'algorithms', speaker_tag: 2\n",
      "word: 'can', speaker_tag: 2\n",
      "word: 'you', speaker_tag: 2\n",
      "word: 'tell', speaker_tag: 2\n",
      "word: 'me', speaker_tag: 2\n",
      "word: 'what', speaker_tag: 2\n",
      "word: 'are', speaker_tag: 2\n",
      "word: 'the', speaker_tag: 2\n",
      "word: 'different', speaker_tag: 2\n",
      "word: 'Frame', speaker_tag: 2\n",
      "word: 'Works', speaker_tag: 2\n",
      "word: 'which', speaker_tag: 2\n",
      "word: 'are', speaker_tag: 2\n",
      "word: 'available', speaker_tag: 2\n",
      "word: 'in', speaker_tag: 2\n",
      "word: 'Dallas', speaker_tag: 2\n",
      "word: 'some', speaker_tag: 2\n",
      "word: 'of', speaker_tag: 2\n",
      "word: 'the', speaker_tag: 2\n",
      "word: 'famous', speaker_tag: 2\n",
      "word: 'JavaScript', speaker_tag: 2\n",
      "word: 'J', speaker_tag: 2\n",
      "word: 'Brooks', speaker_tag: 2\n",
      "word: 'no', speaker_tag: 2\n",
      "word: 'jail', speaker_tag: 2\n",
      "word: 'view', speaker_tag: 2\n",
      "word: 'angler', speaker_tag: 2\n",
      "word: 'Jas', speaker_tag: 2\n",
      "word: 'Andrea', speaker_tag: 2\n",
      "word: 'could', speaker_tag: 2\n",
      "word: 'you', speaker_tag: 2\n",
      "word: 'tell', speaker_tag: 2\n",
      "word: 'me', speaker_tag: 2\n",
      "word: 'the', speaker_tag: 2\n",
      "word: 'founders', speaker_tag: 2\n",
      "word: 'of', speaker_tag: 2\n",
      "word: 'Google', speaker_tag: 2\n",
      "word: 'the', speaker_tag: 2\n",
      "word: 'founders', speaker_tag: 2\n",
      "word: 'of', speaker_tag: 2\n",
      "word: 'Google', speaker_tag: 2\n",
      "word: 'our', speaker_tag: 2\n",
      "word: 'landing', speaker_tag: 2\n",
      "word: 'page', speaker_tag: 2\n",
      "word: 'and', speaker_tag: 2\n",
      "word: 'Sergey', speaker_tag: 2\n",
      "word: 'Brin', speaker_tag: 2\n",
      "1\n",
      "\n",
      "List of questions from the transcript are:\n",
      "\n",
      "['Hello, can you tell me what is machine learning?', 'Can you tell me what are the different frameworks which are available in JavaScript?', 'Could you tell me the founders of Google?']\n",
      "\n",
      "The number of interesting questions from the list of questions are: 0\n",
      "\n",
      "The number of difficult questions from the list of questions are: 2\n"
     ]
    }
   ],
   "source": [
    "demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
